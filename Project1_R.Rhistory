rm(list = ls())
setwd("G:/Tejas/R_practice/Project1_prac")
getwd()
credit_data = read.csv("credit_card_data.csv", header = T, na.strings = c(" ", "", "NA"))
View(credit_data)
str(credit_data)
sum(is.na(credit_data$MINIMUM_PAYMENTS))
sum(is.na(credit_data$CREDIT_LIMIT))
names(credit_data)
#Missing Value Analysis
#Create Dataframe with missing percentage
missing_val = data.frame(apply(credit_data, 2, function(x){sum(is.na(x))}))
View(missing_val)
#Convert row names into column
missing_val$Columns = row.names(missing_val)
row.names(missing_val) = NULL
#Rename the variable name
names(missing_val)[1] = "Missing_Values"
#Calculate Percentage of Missing Values
missing_val$Missing_Percentage = (missing_val$Missing_Values/nrow(credit_data)) * 100
#Write  output result into disk
write.csv(missing_val, "Missing_perc.csv", row.names = F)
x = c("ggplot2", "corrgram", "DMwR", "caret", "randomForest", "unbalanced", "C50", "dummies", "e1071", "Information", "MASS", "rpart", "gbm", "ROSE")
lapply(x, require, character.only = TRUE)
#Imputing Missing Values through Median Method
credit_data[5204, 14]
credit_data$CREDIT_LIMIT[is.na(credit_data$CREDIT_LIMIT)] = median(credit_data$CREDIT_LIMIT, na.rm = T)
credit_data[5204, 14]
#Same we will do for Minimum Payments
credit_data[4, 16]
credit_data$MINIMUM_PAYMENTS[is.na(credit_data$MINIMUM_PAYMENTS)] = median(credit_data$MINIMUM_PAYMENTS, na.rm = T)
credit_data[4, 16]
#So now we have Imputed all the Missing Values
sum(is.na(credit_data))
#Deriving New KPI
# 1. Monthly average purchase and cash advance amount
credit_data$Monthly_Avg_Purchase = credit_data$PURCHASES/credit_data$TENURE
head(credit_data$Monthly_Avg_Purchase)
credit_data$Monthly_cash_advance = credit_data$CASH_ADVANCE/credit_data$TENURE
head(credit_data$Monthly_cash_advance)
sum(credit_data$ONEOFF_PURCHASES == 0 & credit_data$INSTALLMENTS_PURCHASES == 0)
sum(credit_data$ONEOFF_PURCHASES > 0 & credit_data$INSTALLMENTS_PURCHASES > 0)
sum(credit_data$ONEOFF_PURCHASES > 0 & credit_data$INSTALLMENTS_PURCHASES == 0)
sum(credit_data$ONEOFF_PURCHASES == 0 & credit_data$INSTALLMENTS_PURCHASES > 0)
#As per the above details there are 4 types of purchase behaviour in the dataset. We will derive a categorical variable based on their behavior
library(dplyr)
#Function for creating categorical variable
Purchase = function(credit_data)
{
sapply(1:nrow(credit_data), function(i){
if(!credit_data$ONEOFF_PURCHASES[i] && !credit_data$INSTALLMENTS_PURCHASES[i])
return("none")
if(credit_data$ONEOFF_PURCHASES[i] && !credit_data$INSTALLMENTS_PURCHASES[i])
return("oneoff")
if(!credit_data$ONEOFF_PURCHASES[i] && credit_data$INSTALLMENTS_PURCHASES[i])
return("installment")
if(credit_data$ONEOFF_PURCHASES[i] & credit_data$INSTALLMENTS_PURCHASES[i])
return("both_oneoff_installment")
})
}
credit_data$Purchase_Type = Purchase(credit_data)
#We will not be doing the third KPI since it gives a lot of na's
# 4. Usage Limit(balance to credit limit ratio)
#We will not be doing the third KPI since it gives a lot of na's
#Credit Card Utilization
credit_data$Limit_Usage = credit_data$BALANCE/credit_data$CREDIT_LIMIT
head(credit_data$Limit_Usage)
dim(credit_data)
# 5.Payments to minimum payments ratio
credit_data$Paument_minpay = credit_data$PAYMENTS/credit_data$MINIMUM_PAYMENTS
head(credit_data$Paument_minpay)
library(tidyverse)
names(credit_data)[names(credit_data) == "Paument_minpay"] = "Payment_minpay"
str(credit_data)
head(credit_data$Payment_minpay)
sum(is.na(credit_data))
#Outlier Analysis
#Since there are variables having extreme values we will remove the outlier values
dim(credit_data)
credit_outlier = data.frame(credit_data)
credit_outlier = select(credit_outlier, -c(CUST_ID, Purchase_Type))
dim((credit_outlier))
str(credit_outlier)
#As we see all the variables of type numeric
#We know that outlier analysis can only be applied on numeric variables and not on categorical
#Since there are variables having extreme values, so I am doing log-transformation on the dataset to remove outlier effect
hist(credit_outlier$BALANCE)
hist(credit_outlier$PURCHASES, breaks = 20)
hist(credit_outlier$CREDIT_LIMIT)
#All the three histogram plots where right skewed
hist(credit_outlier$TENURE)
#This is left skewed
#We will apply log-transformation to make the skewed data to approximately conform to normality
hist(log(credit_outlier$BALANCE+1))
credit_outlier_log = data.frame(credit_outlier)
#We are doing log-transformation to the entire dataset
credit_outlier_log = (log(credit_outlier_log+1))
view(credit_outlier_log)
library(freqdist)
head(freqdist(credit_data$ONEOFF_PURCHASES), 20)
col = c('BALANCE', 'PURCHASES','CASH_ADVANCE','TENURE','PAYMENTS','MINIMUM_PAYMENTS','PRC_FULL_PAYMENT','CREDIT_LIMIT')
cre_pre = data.frame(credit_outlier_log)
names(cre_pre)
cre_pre = select(credit_outlier_log, -c(BALANCE, PURCHASES, CASH_ADVANCE, TENURE, PAYMENTS, MINIMUM_PAYMENTS, PRC_FULL_PAYMENT, CREDIT_LIMIT))
names(cre_pre)
#Insight on the customer profiles
#Average payment_minpayment ratio for each purchse type.
x = group_by(credit_data, Purchase_Type)
y = summarise(x, mean_pay_minpay = mean(Payment_minpay))
head(y)
#Plotting barplots
barplot(height = y$mean_pay_minpay,  names.arg = y$Purchase_Type, main = "Mean payment_minpayment ratio for each purchse type", horiz = TRUE, col = "blue", border = "red", ylab = "Purchase_Type", xlab = "Frequency")
#customers with installment purchases are paying dues
z = summarise(x, mean_mon_cash_ad = mean(Monthly_cash_advance))
head(z)
barplot(height = z$mean_mon_cash_ad, names.arg = y$Purchase_Type, col = "blue", border = "red", main = "Average cash advance taken by customers of different Purchase type : Both, None,Installment,One_Off", horiz = TRUE, ylab = "Purchase_Type", xlab = "Frequency")
#Customers who don't do either one-off or installment purchases take more cash on advance
w = summarise(x , mean_limit_usage = mean(Limit_Usage))
barplot(height = w$mean_limit_usage, names.arg = y$Purchase_Type, ylab = "Purchase_Type", xlab = "Frequency", col = "blue", border = "red", horiz = TRUE, main = "Customers with Limit Usage")
str(credit_data)
unique(credit_data$Purchase_Type)
b = data.frame(credit_data)
#Creating levels for the Purchase_Type variable
library(dummies)
b$Purchase_Type = as.factor(b$Purchase_Type)
str(b)
b.new = dummy.data.frame(b, names = c("Purchase_Type"), sep = "")
View(b.new)
names(b.new)[21] = "both_oneoff_installment"
names(b.new)[22] = "installment"
names(b.new)[23] = "none"
names(b.new)[24] = "oneoff"
View(b.new)
#Preparing Dataset for Machine Learning Algorithm
l = "Purchase_Type"
dummy1 = select(filter(b.new), c("both_oneoff_installment", "installment", "none", "oneoff"))
View(dummy1)
cr_dummy = data.frame(cre_pre, dummy1)
is.null(cr_dummy)
sum(is.null(cr_dummy))
sum(is.na(cr_dummy))
str(cr_dummy)
library(corrplot)
install.packages("ggcorrplot")
library(ggcorrplot)
ggcorrplot(cor(cr_dummy))
ggcorrplot(cor(cr_dummy), hc.order = TRUE)
cor(cr_dummy)
#Heat map shows that many features are co-related so applying dimensionality reduction will help negating multi-colinearity in data
install.packages("GGally")
library(ggplot2)
library(GGally)
ggcorr(cr_dummy, label = T, label_size = 3, label_round = 2, hjust = 1, size = 3, color = "royalblue", layout.exp = 5, low = "dodgerblue", mid = "gray95", high = "red2", name = "Correlation Heatmap")
#Before applying PCA we will standardize data to avoid effect of scale on our result. Centering and Scaling will make all features with equal weight.
#Standardizing Data
#Making each data of same scale
cr_scaled = scale(cr_dummy)
head(cr_scaled)
#Applying PCA
#Principal Component Analysis is used to reduce the features(Dimentionality reduction)
cr_pca = prcomp(cr_scaled)
cr_pca
summary(cr_pca)
#After looking the summary, we will decide to reduce the dimensionality when the Cumulative Proportion reach 90%
#Here we have 6_components
#The reason why we would like to take the 90% proportion is because that number is considerably able to describe the overall customer information.
library(factoextra)
#Visualization on PCA
fviz_pca_biplot(cr_pca, axes = c(1:2), col.var = "orange", col.ind = "rolayblue",
labelsize = 3) +
theme_gray() +
labs(title = "Biplot of PC1 and PC2")
#Visualization on PCA
fviz_pca_biplot(cr_pca, axes = c(1:2), col.var = "orange", col.ind = "red", labelsize = 3, select.ind = list(contrib = 5)) +
theme_grey() +
labs(title = "Outlier of PC1 and PC2")
#As seen on the Outlier Plot, there are 5 Outliers with the following conditions:
credit_outlier[c("1257", "1605", "3938", "6952", "8316"), ]
#1. Customers 1257, 1605 and 3938 have a very high PURCHASES and ONEOFF_PUCHASES.
#2. Customer 6952: Have a very high Limit_Usage.
#3. Customer 8316: Have a very high CASH_ADVANCE
#variable Data Plot
fviz_pca_var(cr_pca, col.var = "orange") +
theme_gray()+
labs(title = "Variable Factor Map - PC_1 and PC_2")
View(cr_dummy)
#Some Interpretations:
#1. As we can see in the Variable Data Plot, the Monthly_Avg_Purchase and PURCHASES_TRX variables are close to the circle, indicating that those variables are highly contributed to the PC1 and PC2.
#2. On the other hand, the BALANCE_FREQUENCY, one_off and Payment_minpay are variables that less contributed to PC1 and PC2.
#Dimentionality Reduction
view(cr_pca$x)
credit_final = cr_pca$x[, 1:6]
View(credit_final)
#Based on interepertations from above, we will decide to take only 6 dimensions and put to new dataset called credit_final
head(credit_final)
#K-Means Clustering
#Elbow Method
fviz_nbclust(credit_final,
kmeans, method = "wss",
linecolor = "blue") +
geom_vline(xintercept = c(4, 7), linetype = 2, col = "red") +
theme_gray()
#As we can see there is less difference between cluster 6 and 7 even though there is again a dip in the graph
#we can ignore cluster 7 and continue examining cluster 4, 5 and 6
#K-Means Clustering
for(i in 4:6){
set.seed(289)
model = kmeans(credit_final, i)
print(paste("WSS of K", i, "=", model$tot.withinss))
print(paste("BSS Proportion of K", i, "=", model$betweenss/model$totss))
print(paste("Cluster Size of K", i, "="))
print(paste(model$size))
print(fviz_cluster(model, credit_outlier, palette = "Set1") +
theme_gray())
}
credit_km4 = kmeans(credit_final, 4)
credit_outlier$CLUSTER = credit_km4$cluster
head(credit_outlier, 10)
view(credit_outlier)
#Performance metrics also suggest that K-means with 4 cluster is able to show distinguished characteristics of each cluster
#so we stop with the cluster 4
savehistory(file = "Final_code.Rhistory")
